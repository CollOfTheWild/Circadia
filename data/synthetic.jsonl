{"messages": [{"role": "user", "content": "Summarize the goal of the circadian rhythm experiment in one sentence."}, {"role": "assistant", "content": "We log daytime chats and run nightly QLoRA on the 4-bit Llama model to slowly adapt it on-device."}]}
{"messages": [{"role": "user", "content": "List three safety considerations when fine-tuning locally."}, {"role": "assistant", "content": "- Keep model weights and logs local\n- Avoid sensitive data in prompts\n- Monitor memory/thermal limits on the Mac Mini"}]}
{"messages": [{"role": "user", "content": "Give a JSON plan with keys 'collect', 'train', 'eval' for the nightly workflow."}, {"role": "assistant", "content": "{\"collect\":\"log all chat turns to data/memories.jsonl\",\"train\":\"run sleep.py to fine-tune adapters in adapters/nightly_update\",\"eval\":\"run eval_loss.py and eval_harness.py pre/post to track loss and capability\"}"}]}
{"messages": [{"role": "user", "content": "What is QLoRA and why is it used here?"}, {"role": "assistant", "content": "QLoRA trains low-rank adapters on quantized weights, keeping memory and compute small enough for the M2 16GB box."}]}
{"messages": [{"role": "user", "content": "Return a bullet list of two advantages of using mlx-lm on Apple Silicon."}, {"role": "assistant", "content": "- Uses Metal-accelerated kernels optimized for M-series GPUs\n- Avoids CUDA overhead and fits small quantized models in 16GB RAM"}]}
{"messages": [{"role": "user", "content": "In one sentence, explain how adapters are applied during chat."}, {"role": "assistant", "content": "At chat start, if adapters/nightly_update contains adapter_config.json, the model loads those LoRA deltas on top of the base weights."}]}
{"messages": [{"role": "user", "content": "Provide a concise JSON with fields model, quantization, batch_size, lora_rank for this project."}, {"role": "assistant", "content": "{\"model\":\"mlx-community/Llama-3.2-3B-Instruct-4bit\",\"quantization\":\"4bit\",\"batch_size\":1,\"lora_rank\":\"n/a in current CLI\"}"}]}
{"messages": [{"role": "user", "content": "Give two brief prompts that would be good for daily logging."}, {"role": "assistant", "content": "1) \"Summarize today’s troubleshooting steps in three bullets.\"\n2) \"Draft a one-sentence changelog for today’s adapter update.\""}]}
{"messages": [{"role": "user", "content": "Explain why we keep batch size and iters low on this hardware."}, {"role": "assistant", "content": "To avoid OOM and thermal throttling on a 16GB M2; small batch/iters keeps peak memory and runtime manageable."}]}
{"messages": [{"role": "user", "content": "Output a short checklist for running run_cycle.py."}, {"role": "assistant", "content": "- Collect chats in data/memories.jsonl\n- Ensure no existing adapter if starting fresh\n- Run run_cycle.py with a tag\n- Review loss CSV and eval summaries\n- Inspect adapters/nightly_update outputs"}]}
{"messages": [{"role": "user", "content": "Write a one-sentence reminder about not overfitting on tiny data."}, {"role": "assistant", "content": "Avoid long training runs on very few chats; gather more turns or add synthetic data before another sleep cycle."}]}
{"messages": [{"role": "user", "content": "Return a brief JSON object with keys 'path' and 'note' describing the synthetic dataset."}, {"role": "assistant", "content": "{\"path\":\"data/synthetic.jsonl\",\"note\":\"starter synthetic seed for QLoRA when chat logs are tiny\"}"}]}

